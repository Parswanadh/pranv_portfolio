export interface Project {
  slug: string
  title: string
  description: string
  longDescription?: string
  howItWorks?: string
  keyFeatures?: string[]
  category: string
  status: string
  techStack: string[]
  year: string
  period: string
}

export const projects: Project[] = [
  {
    slug: 'auto-git-publisher',
    title: 'AUTO-GIT Publisher – Autonomous Research-to-Code Pipeline',
    description: 'Transforms AI research papers into production-ready code repositories using 12 specialized agents.',
    longDescription: 'AUTO-GIT Publisher is an agentic AI system that autonomously transforms research papers from arXiv into fully-functional, production-ready code repositories. By orchestrating 12 specialized agents—including paper retrieval, code generation, documentation, testing, and repository management—it completely automates the research-to-code workflow.\n\nThe system begins by extracting research papers from arXiv, analyzing their methodology and contributions. It then generates implementation code, writes comprehensive documentation, creates test suites, and initializes Git repositories with proper structure and metadata. Each agent specializes in a specific task, working cooperatively through a LangGraph-based orchestration layer.\n\nPowered by Groq API for lightning-fast inference and SBERT embeddings for semantic understanding, AUTO-GIT Publisher bridges the gap between academic research and practical implementation. It enables researchers and developers to quickly validate and implement cutting-edge algorithms without manual coding effort.',
    howItWorks: '1. Paper Discovery: The Research Agent scans arXiv for recent papers matching specified topics or keywords.\n\n2. Content Extraction: The Parser Agent downloads and extracts full text, methodology sections, and algorithms from the selected paper.\n\n3. Semantic Analysis: SBERT embeddings analyze the paper to identify core contributions, algorithms, and implementation requirements.\n\n4. Code Planning: The Planning Agent breaks down the paper into implementable components and creates a development roadmap.\n\n5. Code Generation: The Coder Agent generates production-quality implementation code based on the extracted algorithms.\n\n6. Documentation: The Docs Agent creates README files, inline comments, and API documentation.\n\n7. Testing: The Tester Agent writes unit tests and validates generated code against paper specifications.\n\n8. Repository Initialization: The Git Agent initializes a Git repository with proper structure, LICENSE, and .gitignore.\n\n9. Version Control: Automatically commits code with meaningful messages following conventional commit format.\n\n10. Quality Assurance: Review Agent performs final checks for code quality, documentation completeness, and test coverage.',
    keyFeatures: [
      '12 Specialized Agents: Each agent handles a specific task (research, parsing, coding, testing, documentation)',
      'LangGraph Orchestration: Sophisticated workflow management enables agents to collaborate and share context',
      'arXiv Integration: Direct access to latest research papers with automatic metadata extraction',
      'Semantic Understanding: SBERT embeddings comprehend paper content and identify implementable algorithms',
      'Production-Ready Output: Generates complete repositories with tests, docs, and proper Git structure',
      'Groq-Powered: Ultra-fast inference enables rapid code generation from complex papers'
    ],
    category: 'Agentic AI',
    status: 'Active Development',
    techStack: ['Python', 'Groq API', 'LangGraph', 'arXiv API', 'PyTorch', 'SBERT'],
    year: '2025',
    period: '2025 — Present',
  },
  {
    slug: 'pro-code',
    title: 'PRO_CODE – Complete Local AI Coding Assistant',
    description: 'Privacy-focused AI coding assistant running entirely on local machine with persistent memory and tool calling.',
    longDescription: 'PRO_CODE is a privacy-focused AI coding assistant that runs entirely on your local machine. It combines the power of local Large Language Models (LLMs) with persistent memory capabilities, providing intelligent code assistance without ever sending your code to external servers.\n\nThe system uses ChromaDB for vector-based memory storage, enabling it to remember context from previous sessions and learn your coding patterns over time. With Ollama integration running CodeLlama 13B locally, you get fast, private code generation with no API costs or privacy concerns.\n\nBuilt with Python and featuring a beautiful CLI interface powered by Rich and Typer, PRO_CODE provides an elegant development experience. Whether you\'re working on proprietary codebases, prefer offline development, or simply want more control over your AI tools, PRO_CODE offers a powerful alternative to cloud-based solutions.',
    howItWorks: '1. Query Input: User provides coding question or task through the CLI interface.\n\n2. Vector Embedding: The query is converted to vector embeddings using sentence transformers.\n\n3. Context Retrieval: ChromaDB performs semantic search to retrieve relevant past interactions and code patterns.\n\n4. Context Assembly: Retrieved embeddings, current file contents, and conversation history are combined into a rich prompt.\n\n5. Local Inference: Ollama runs CodeLlama 13B locally to generate code suggestions and answers.\n\n6. Streaming Response: Results are streamed back in real-time with beautiful formatting and syntax highlighting.\n\n7. Memory Storage: Successful interactions are embedded and stored in ChromaDB for future retrieval.\n\n8. Tool Execution: File operations, Git commands, and terminal operations are executed with explicit approval.',
    keyFeatures: [
      '100% Local Operation: No data leaves your machine, ensuring complete privacy and security',
      'Persistent Memory: ChromaDB vector database remembers context across sessions using semantic embeddings',
      'High Context Awareness: Leverages vector search to retrieve relevant past solutions and coding patterns',
      'Tool Calling: Execute file operations, Git commands, and terminal operations with safety confirmations',
      'Offline Capable: Works without internet connection once models are downloaded',
      'Beautiful CLI: Rich terminal UI with syntax highlighting, progress bars, and markdown rendering'
    ],
    category: 'AI Tools',
    status: 'Production-Ready',
    techStack: ['Python', 'Ollama', 'ChromaDB', 'Sentence Transformers', 'Typer', 'Rich'],
    year: '2025',
    period: '2025 — Present',
  },
  {
    slug: 'gpt-oss-vision',
    title: 'GPT-OSS Vision – Multimodal AI Pipeline with Q-Former',
    description: 'First implementation of trainable Q-Former with GPT-OSS for satellite imagery analysis. Faced significant challenges in adapter training and domain adaptation.',
    longDescription: 'GPT-OSS Vision was an ambitious research experiment attempting to bridge text-based Large Language Models with visual understanding. The project aimed to enable a 20B parameter text-only LLM (GPT-OSS) to understand and analyze satellite imagery through a novel Q-Former adapter architecture.\n\nWhile the project did not achieve production-ready results due to significant technical challenges, it represents valuable research in multimodal AI integration and adapter-based learning approaches. The experience directly informed my approach to current projects, emphasizing the importance of trainable models over massive frozen architectures.\n\nKey challenges included training instability, Q-Former convergence issues, domain overfitting to satellite imagery, and the fundamental limitations of adapter-only fine-tuning for models above 10B parameters. These learnings are now applied to AUTO-GIT and PRO_CODE, where I prioritize end-to-end optimization and extensive evaluation.',
    howItWorks: '1. Image Input: Satellite imagery (224×224×3) is fed into the Remote CLIP vision encoder.\n\n2. Vision Encoding: Remote CLIP (specialized for satellite imagery) converts the image into 49 patch embeddings of 768 dimensions each.\n\n3. Q-Former Compression: A trainable Q-Former with 32 learnable query tokens performs cross-attention with visual patches, compressing features while preserving semantic information.\n\n4. Adapter Projection: A linear layer with LoRA fine-tuning projects the 768-dimensional Q-Former outputs to 2048 dimensions (GPT-OSS embedding space).\n\n5. Token Integration: Compressed visual tokens are prepended to the text prompt as special "visual tokens" for the language model.\n\n6. Language Generation: GPT-OSS 20B (frozen) generates natural language descriptions, VQA answers, or classifications based on the visual context.\n\n7. Training Loop: Only the Q-Former and adapter layers are trained using cross-entropy loss with gradient accumulation.',
    keyFeatures: [
      'Novel Architecture: First trainable Q-Former implementation with GPT-OSS 20B',
      'Remote CLIP Integration: Specialized vision encoder for satellite imagery outperforms standard CLIP',
      'LoRA Fine-Tuning: Efficient adapter training using low-rank adaptation to reduce parameters',
      'Domain-Specific: Focused on challenging satellite imagery analysis with atmospheric effects',
      'Research Transparency: All failures, training logs, and analyses documented openly',
      '3D Visualization: Interactive architecture viewer demonstrating the complete pipeline'
    ],
    category: 'Multimodal AI',
    status: 'Research Completed',
    techStack: ['Python', 'LLaVA 7B', 'GPT-OSS 20B', 'Q-Former', 'Remote CLIP', 'PyTorch', 'Transformers'],
    year: '2025',
    period: 'Jan — Feb 2025',
  },
  {
    slug: 'parshu-stt',
    title: 'Parshu-STT – Real-Time Voice Transcription',
    description: 'Lightweight always-on voice transcription for Windows with global hotkey and auto-paste.',
    longDescription: 'Parshu-STT is a lightweight, always-on voice transcription tool designed specifically for Windows. It transforms spoken words into text in real-time and automatically pastes the transcription at your cursor position using a global hotkey.\n\nBuilt with production-ready architecture, this tool is used daily for hands-free typing and documentation tasks. The combination of Groq Whisper v3 Turbo for ultra-fast transcription, FFmpeg for audio capture, and PyQt6 for native Windows integration creates a seamless productivity experience.\n\nWhat makes Parshu-STT unique is its workflow-focused design: custom voice commands like "nextline" (inserts newline) and "and" (inserts comma with space) enable natural dictation flow without breaking concentration. Works universally across all applications—text editors, browsers, IDEs, and more.',
    howItWorks: '1. Activation: User presses global hotkey (Ctrl+Shift+V) from any application to activate recording.\n\n2. Audio Capture: FFmpeg begins capturing real-time audio from the default microphone at 16kHz sample rate optimized for Whisper.\n\n3. Recording Indicator: Visual notification shows recording status while user speaks.\n\n4. Streaming to API: Audio is buffered and sent to Groq\'s Whisper v3 Turbo API for low-latency transcription.\n\n5. Real-Time Transcription: Groq API returns transcribed text within 1-2 seconds with automatic punctuation and formatting.\n\n6. Command Processing: Special keywords ("nextline", "and") are replaced with their corresponding characters.\n\n7. Auto-Paste: Transcribed text is automatically pasted at the current cursor position using clipboard simulation.\n\n8. Ready State: System returns to idle state, ready for next dictation session.',
    keyFeatures: [
      'Global Hotkey: Works from any application with system-wide hotkey registration',
      'Auto-Paste: Automatically inserts text at cursor position without manual copy-paste',
      'Custom Commands: "nextline" for newlines, "and" for commas—extensible design for more commands',
      'Universal Compatibility: Works with all text input fields across any application',
      'Groq Whisper v3 Turbo: Industry-leading transcription with <2 second latency',
      'Windows-Native: System tray integration, auto-startup option, minimal resource footprint'
    ],
    category: 'Productivity Tools',
    status: 'Production-Ready',
    techStack: ['Python', 'Electron 28', 'Groq Whisper v3 Turbo', 'FFmpeg', 'PyQt6'],
    year: '2025',
    period: '2025 — Present',
  },
  {
    slug: 'whisper-stt',
    title: 'WhisperSTT – AI Speech-to-Text Assistant',
    description: 'Real-time transcription tool using Whisper V3 Turbo with hybrid local/GROQ API modes.',
    longDescription: 'WhisperSTT is a real-time transcription tool that leverages Whisper V3 Turbo for accurate speech-to-text conversion. The tool supports both local processing and GROQ API modes, giving users flexibility between privacy and speed.\n\nBuilt with a Tkinter GUI for easy interaction, WhisperSTT can process audio from microphone input or pre-recorded files. The hybrid mode allows users to switch between local inference (for complete privacy) and GROQ API (for maximum speed) based on their needs.\n\nThis project represents an evolution in my STT tools, incorporating lessons learned from Parshu-STT while adding more configuration options and a traditional GUI interface.',
    howItWorks: '1. Mode Selection: User chooses between local Whisper V3 or GROQ API mode based on privacy/speed preferences.\n\n2. Audio Input: Select microphone for live recording or load audio file for batch transcription.\n\n3. Processing: Audio is sent to selected inference engine (local model or GROQ API).\n\n4. Transcription: Whisper V3 Turbo processes audio and returns text with punctuation and formatting.\n\n5. Display: Transcribed text appears in real-time in the GUI interface.\n\n6. Export: Option to save transcripts to text files for documentation.',
    keyFeatures: [
      'Hybrid Mode: Switch between local processing and GROQ API based on needs',
      'Turbo Performance: Whisper V3 Turbo provides faster inference without accuracy loss',
      'Real-Time Display: Live transcription updates in GUI interface',
      'File Support: Process pre-recorded audio files in addition to live recording',
      'Export Options: Save transcripts to text files for documentation',
      'Privacy Mode: Local processing option ensures audio never leaves your machine'
    ],
    category: 'Generative AI',
    status: 'Active',
    techStack: ['Python', 'Whisper V3', 'Tkinter', 'GROQ API'],
    year: '2025',
    period: 'Mar 2025 — Present',
  },
  {
    slug: 'cli-tour',
    title: 'CLI-Tour – Personalized AI Terminal Assistant',
    description: 'CLI-based LLM assistant to edit and recall projects from the terminal with memory retention.',
    longDescription: 'CLI-Tour is a CLI-based LLM assistant that enables editing and recalling projects directly from the terminal with persistent memory retention. Designed for developers who prefer terminal workflows and want AI assistance without leaving their command-line environment.\n\nThe system uses Ollama for local LLM inference and maintains project context across sessions using vector embeddings. This allows CLI-Tour to remember your project structure, coding patterns, and past conversations for more contextual assistance.\n\nPerfect for quick code edits, project documentation, or recalling implementation details without breaking your development flow.',
    howItWorks: '1. Project Indexing: CLI-Tour scans and indexes local project files and directory structure.\n\n2. Memory Storage: Project context and patterns are stored as vector embeddings for semantic retrieval.\n\n3. Terminal Interaction: User interacts through natural language commands in the CLI.\n\n4. Context Retrieval: Vector search retrieves relevant past conversations and code patterns.\n\n5. Local Inference: Ollama runs local LLM to generate responses based on retrieved context.\n\n6. Code Editing: Can suggest and apply code changes directly to project files.',
    keyFeatures: [
      'Persistent Memory: Remembers project context across sessions using vector embeddings',
      'Local Processing: Runs entirely offline using Ollama for privacy',
      'Code Editing: Can suggest and apply code changes to project files',
      'Project Recall: Quickly switch between and recall details of different projects',
      'Natural Language: Interact through conversational commands instead of memorizing CLI syntax',
      'Terminal-Native: Designed for developers who prefer command-line workflows'
    ],
    category: 'AI Tools',
    status: 'Ongoing',
    techStack: ['Python', 'LLM', 'Ollama', 'CLI'],
    year: '2025',
    period: 'Jul 2025 — Ongoing',
  },
  {
    slug: 'multimodal-adapter',
    title: 'Multimodal Adapter Research – LLM + Vision Integration',
    description: 'Experimenting with GPT-20B and LLaVA-7B to enable image reasoning with adapter layers.',
    longDescription: 'Research project experimenting with GPT-20B and LLaVA-7B to enable image understanding through trainable Q-Former adapter layers. Explored training adapters for bridging vision and language models without end-to-end fine-tuning.\n\nThis was a precursor to the more ambitious GPT-OSS Vision project, focusing on understanding the challenges of adapter-based multimodal integration. Key learnings about training stability and domain adaptation directly informed subsequent research.',
    howItWorks: '1. Model Loading: Load pre-trained GPT-20B (language) and LLaVA-7B (vision encoder).\n\n2. Adapter Creation: Implement trainable Q-Former layers for cross-modal alignment.\n\n3. Feature Fusion: Bridge vision embeddings from CLIP with language model tokens through adapter.\n\n4. Training: Fine-tune adapters on target dataset while keeping base models frozen.\n\n5. Inference: Enable image understanding through text prompts using adapted model.',
    keyFeatures: [
      'Q-Former Architecture: Trainable adapter layers for vision-language alignment',
      'Remote CLIP: Vision encoder specialized for image feature extraction',
      'Domain Adaptation: Training focused on specific domain (satellite imagery)',
      'Research Implementation: Early exploration of adapter-based multimodal approaches',
      'Cross-Modal Fusion: Novel approach to combining heterogeneous model architectures',
      'Frozen Base Models: Training efficiency by keeping large models frozen'
    ],
    category: 'Research',
    status: 'Completed',
    techStack: ['PyTorch', 'LLaVA', 'GPT', 'Transformers'],
    year: '2025',
    period: 'Jan — Feb 2025',
  },
  {
    slug: 'raspberry-pi-vision',
    title: 'Raspberry Pi Vision Model – Emergency Vehicle Detection',
    description: 'Deployed YOLO on Raspberry Pi for real-time emergency vehicle recognition.',
    longDescription: 'Deployed YOLO (You Only Look Once) object detection model on Raspberry Pi for real-time emergency vehicle recognition. Demonstrates edge AI capabilities for practical safety applications.\n\nThe project involved training YOLOv5 on an emergency vehicle dataset, optimizing the model for Raspberry Pi 4 constraints, and implementing real-time inference with alert system. Achieves 87% mAP accuracy while processing video at 10-15 FPS on edge hardware.\n\nThis project proved that lightweight models can run effectively on constrained hardware, opening possibilities for privacy-preserving AI at the edge.',
    howItWorks: '1. Model Training: YOLOv5 trained on emergency vehicle dataset (ambulances, fire trucks, police cars).\n\n2. Edge Optimization: Model quantized and optimized for Raspberry Pi 4 memory and compute constraints.\n\n3. Camera Setup: USB camera connected for real-time video capture at 15 FPS.\n\n4. Inference Pipeline: Each frame processed through YOLO for object detection (~80ms per frame).\n\n5. Detection Logic: Filter for emergency vehicle classes with confidence thresholding.\n\n6. Alert System: Trigger visual and audio alerts upon emergency vehicle detection.',
    keyFeatures: [
      'Real-Time Detection: Processes video at 10-15 FPS on edge device',
      'Model Optimization: Quantized model for Pi\'s limited compute and memory',
      'Emergency Vehicles: Detects ambulances, fire trucks, and police cars',
      'Alert System: Visual and audio alerts triggered on detection',
      'Edge AI: Complete inference without cloud dependency for privacy',
      'Low Power: ~5W power consumption during inference'
    ],
    category: 'Embedded Systems',
    status: 'Completed',
    techStack: ['Raspberry Pi', 'YOLO', 'Python', 'OpenCV'],
    year: '2024',
    period: 'Nov — Dec 2024',
  },
  {
    slug: 'ai-robotic-arm',
    title: 'AI-Controlled Robotic Arm',
    description: 'Camera-guided robotic arm that sorts objects via AI classification.',
    longDescription: 'Camera-guided robotic arm that autonomously sorts objects based on AI classification. Combines computer vision, robotics, and machine learning for automated sorting tasks.\n\nThe system uses a CNN to identify objects on a conveyor belt, maps their coordinates to robotic arm positions, plans optimal trajectories, and actuates servo motors to sort objects into appropriate bins. Achieves 93% sorting accuracy with 1.8 second cycle time.\n\nThis project taught me about the practical challenges of integrating AI with physical systems—calibration, timing synchronization, and environmental factors all significantly impact performance.',
    howItWorks: '1. Vision Input: Camera captures images of objects on conveyor belt.\n\n2. Object Detection: CNN model identifies object category and bounding box.\n\n3. Coordinate Mapping: Image coordinates transformed to robot arm coordinate space.\n\n4. Path Planning: Calculate optimal movement trajectory to target object.\n\n5. Servo Actuation: 4-DOF arm moves along planned path using servo motors.\n\n6. Sorting: Object placed in appropriate bin based on classification.\n\n7. Error Recovery: Automatic retry on failed grasps.',
    keyFeatures: [
      'Computer Vision: Object detection and classification pipeline using CNN',
      'Robotic Control: 4-DOF arm with servo motor actuation',
      'Real-Time Processing: Classifies and sorts objects in <2 seconds',
      'Multiple Categories: Sorts objects by color, shape, or type',
      'Calibration System: Automatic coordinate mapping between camera and arm',
      'Safety Features: Obstacle detection and force limiting for safe operation'
    ],
    category: 'Robotics',
    status: 'Completed',
    techStack: ['Python', 'Computer Vision', 'Arduino', 'Servo Motors'],
    year: '2024',
    period: 'Oct 2024 — Feb 2025',
  },
  {
    slug: 'spinlaunch-prototype',
    title: 'SpinLaunch Prototype – Satellite Launch Simulation',
    description: 'Spin-based launch prototype demonstrating fuel-efficient orbital insertion.',
    longDescription: 'Spin-based launch prototype demonstrating fuel-efficient orbital insertion through kinetic energy accumulation. Simulates the innovative approach to reducing rocket fuel requirements by spinning payload to extreme velocities before release.\n\nThe project involves physics simulation of the spin chamber, energy accumulation calculations, release mechanism timing, and atmospheric trajectory modeling. CAD modeling of prototype components and custom physics engine for launch dynamics complete the system.\n\nTheoretical advantages include 70% reduction in launch fuel, lower costs, reusable launch system, and smaller environmental footprint. However, real-world validation is required to confirm simulation accuracy.',
    howItWorks: '1. Spin Chamber: Payload rotated to extreme velocities (7500+ m/s) in vacuum chamber.\n\n2. Energy Accumulation: Kinetic energy builds through rotational motion (KE = ½mv²).\n\n3. Release Timing: Precisely timed release directs payload to target orbit.\n\n4. Vacuum Exit: Payload exits chamber through specialized airlock system.\n\n5. Atmospheric Trajectory: Model drag and trajectory through atmosphere to space.\n\n6. Orbital Insertion: Calculate required velocity for target altitude and orbit.',
    keyFeatures: [
      'Kinetic Launch: Eliminates first-stage rocket fuel requirements',
      'Vacuum Chamber: Simulates friction-free spin environment',
      'Trajectory Optimization: Calculates optimal release angles and timing',
      'Fuel Efficiency: Potential 70% reduction in launch fuel',
      'CAD Design: 3D modeled prototype components',
      'Physics Engine: Custom simulation for launch dynamics'
    ],
    category: 'Space Tech',
    status: 'Active',
    techStack: ['Physics', 'Simulation', 'CAD'],
    year: '2024',
    period: 'Jul 2024 — Present',
  },
  {
    slug: 'krishi-setu',
    title: 'Krishi Setu – Farmers Service Platform',
    description: 'Flask + MongoDB platform with ML-based crop price prediction for farmers.',
    longDescription: 'Krishi Setu (Farmers Bridge) is a comprehensive platform designed to empower farmers with data-driven insights and market intelligence. The system combines machine learning-based crop price prediction with a user-friendly interface to help farmers make informed decisions about planting, harvesting, and selling their crops.\n\nThe ML models analyze historical price data, weather patterns, seasonal trends, and market demand to predict future crop prices with accuracy. Farmers can access price forecasts, market trends, and connect directly with buyers through the platform.\n\nBuilt with Flask for the backend and MongoDB for data storage, the platform provides real-time recommendations and personalized insights based on crop type, location, and season. The TensorFlow and Scikit-learn models power the prediction engine, processing thousands of data points to generate actionable forecasts.',
    howItWorks: '1. Data Collection: Historical crop prices, weather data, and market trends collected from various sources.\n\n2. Feature Engineering: Extract relevant features including seasonal patterns, weather variables, and market indicators.\n\n3. Model Training: TensorFlow and Scikit-learn models trained on historical data to learn price patterns.\n\n4. Price Prediction: Models generate forecasts for different crops based on current market conditions.\n\n5. User Interface: Farmers access predictions and recommendations through Flask web interface.\n\n6. Database: MongoDB stores user profiles, historical data, and prediction results.\n\n7. Buyer Connection: Platform connects farmers with potential buyers for direct transactions.',
    keyFeatures: [
      'ML Price Prediction: Advanced models predict future crop prices using historical data',
      'Market Insights: Real-time market trends and demand analysis',
      'Weather Integration: Weather patterns factored into price predictions',
      'Direct Buyer Connection: Farmers can connect directly with buyers',
      'User Profiles: Personalized recommendations based on crop type and location',
      'Responsive Design: Mobile-friendly interface accessible in rural areas'
    ],
    category: 'Web Development',
    status: 'Completed',
    techStack: ['Flask', 'MongoDB', 'TensorFlow', 'Scikit-learn'],
    year: '2024',
    period: '2024',
  },
  {
    slug: 'ipo-insights',
    title: 'IPO Insights – Big Data Analytics Application',
    description: 'Apache Spark + MongoDB + Streamlit platform for comprehensive IPO analysis.',
    longDescription: 'IPO Insights is a big data analytics application designed to analyze Initial Public Offering (IPO) data at scale. The platform processes massive datasets containing historical IPO performance, company financials, market conditions, and post-IPO stock trends to generate actionable insights for investors.\n\nLeveraging Apache Spark for distributed data processing, the system can analyze thousands of IPO records and millions of related data points efficiently. MongoDB provides flexible storage for structured and unstructured IPO data, while Streamlit creates an interactive interface for visualizing complex trends and patterns.\n\nInvestors can explore IPO performance by sector, market conditions, company size, and timing. The platform identifies patterns in underpricing, long-term performance, and market sentiment to help make data-driven investment decisions.',
    howItWorks: '1. Data Ingestion: IPO data collected from multiple sources including SEC filings and market data providers.\n\n2. Spark Processing: Apache Spark clusters process massive datasets in parallel for analysis.\n\n3. Feature Extraction: Extract key features like company size, sector, market conditions, and financial metrics.\n\n4. Analytics Pipeline: Spark jobs calculate aggregations, correlations, and statistical patterns.\n\n5. Data Storage: Processed data and results stored in MongoDB for fast retrieval.\n\n6. Visualization: Streamlit dashboard presents insights through interactive charts and graphs.\n\n7. Query Interface: Users can filter, explore, and analyze IPO data based on various criteria.',
    keyFeatures: [
      'Big Data Processing: Apache Spark handles massive IPO datasets efficiently',
      'Interactive Dashboard: Streamlit provides intuitive visualization of complex data',
      'Historical Analysis: Analyze IPO performance trends over years',
      'Sector Comparison: Compare IPO performance across different industry sectors',
      'Market Insights: Identify patterns in IPO pricing and long-term performance',
      'Scalable Architecture: Distributed processing enables handling growing datasets'
    ],
    category: 'Data Science',
    status: 'Completed',
    techStack: ['Apache Spark', 'MongoDB', 'Streamlit'],
    year: '2024',
    period: '2024',
  },
  {
    slug: 'assistive-navigation',
    title: 'Assistive Navigation System using RealSense, YOLOv8, and ROS2',
    description: 'Real-time obstacle detection system for visually impaired using depth sensing and AI.',
    longDescription: 'The Assistive Navigation System is a robotics project designed to help visually impaired individuals navigate safely through indoor and outdoor environments. The system combines Intel RealSense depth cameras with YOLOv8 object detection and ROS2 (Robot Operating System 2) to create a comprehensive navigation assistant.\n\nUsing RTAB (Real-Time Appearance-Based) mapping and SLAM (Simultaneous Localization and Mapping), the system builds 3D maps of the environment while tracking the user position in real-time. YOLOv8 identifies obstacles, hazards, and objects of interest, while the RealSense camera provides accurate depth measurements for distance estimation.\n\nThe system provides audio feedback through bone conduction headphones, warning users of obstacles and guiding them along safe paths. ROS2 enables modular architecture, making it easy to integrate additional sensors and functionality.',
    howItWorks: '1. Depth Sensing: Intel RealSense camera captures RGB-D data (color + depth information).\n\n2. Object Detection: YOLOv8 processes camera feed in real-time to detect obstacles, hazards, and objects.\n\n3. Distance Estimation: Depth data provides accurate distance measurements to detected objects.\n\n4. SLAM: RTAB-SLAM builds 3D map of environment and tracks user position.\n\n5. Path Planning: ROS2 navigation stack plans safe paths around detected obstacles.\n\n6. Audio Feedback: Bone conduction headphones provide spatial audio warnings and guidance.\n\n7. Localization: System maintains user position within mapped environment.\n\n8. Obstacle Avoidance: Real-time alerts for immediate hazards in navigation path.',
    keyFeatures: [
      'Real-Time Detection: YOLOv8 identifies obstacles at 30+ FPS for immediate response',
      'Depth Sensing: RealSense provides accurate distance measurements for spatial awareness',
      '3D Mapping: RTAB-SLAM builds and updates environment maps in real-time',
      'Audio Guidance: Spatial audio feedback for intuitive navigation assistance',
      'Modular Architecture: ROS2 enables easy integration of additional sensors',
      'Indoor/Outdoor: Works in various environments with automatic mode switching'
    ],
    category: 'Robotics',
    status: 'Completed',
    techStack: ['RealSense', 'YOLOv8', 'ROS2', 'RTAB-SLAM'],
    year: '2024',
    period: '2024',
  },
  {
    slug: 'distraction-monitoring',
    title: 'Vision-Based Distraction Monitoring System',
    description: 'Edge-based monitoring system with head pose and eye-state tracking for safety.',
    longDescription: 'The Vision-Based Distraction Monitoring System is a computer vision application designed to detect and alert users of distraction or fatigue in real-time. Using edge-based processing, the system tracks head pose, eye state, and facial landmarks to determine attention levels without requiring cloud connectivity.\n\nThe system uses advanced computer vision algorithms to estimate head orientation, detect eye openness/closure, track gaze direction, and identify signs of drowsiness or distraction. Multithreading ensures smooth performance even when running on resource-constrained edge devices.\n\nPotential applications include driver safety systems, workplace monitoring, educational settings, and fatigue detection for heavy machinery operators. The lightweight architecture enables deployment on various edge devices without requiring powerful hardware.',
    howItWorks: '1. Video Capture: Camera feed captured in real-time from webcam or video source.\n\n2. Face Detection: OpenCV detects face and extracts facial landmarks (68 points).\n\n3. Head Pose Estimation: Calculate head rotation (pitch, yaw, roll) from landmark positions.\n\n4. Eye State Analysis: Detect eye openness, closure, and blink frequency.\n\n5. Gaze Tracking: Estimate direction of gaze based on eye and head orientation.\n\n6. Distraction Detection: Analyze patterns to determine if user is distracted or drowsy.\n\n7. Alert Generation: Trigger visual/audible alerts when distraction thresholds exceeded.\n\n8. Logging: Record events and metrics for analysis and improvement.',
    keyFeatures: [
      'Real-Time Tracking: Monitors head pose and eye state at 30+ FPS',
      'Edge-Based Processing: Runs locally without cloud dependency for privacy',
      'Multithreaded Architecture: Efficient use of CPU resources for smooth performance',
      'Lightweight: Optimized to run on resource-constrained edge devices',
      'Multiple Metrics: Tracks head pose, eye state, blink rate, and gaze direction',
      'Configurable Alerts: Customizable thresholds for distraction and fatigue detection'
    ],
    category: 'Computer Vision',
    status: 'Completed',
    techStack: ['OpenCV', 'Python', 'Multithreading'],
    year: '2024',
    period: '2024',
  },
  {
    slug: 'llm-aws-deployment',
    title: 'LLM Deployment on AWS EC2',
    description: 'Deployed Large Language Model using OpenMP+MPI parallelism for distributed inference.',
    longDescription: 'This project involved deploying a Large Language Model (LLM) on AWS EC2 instances using advanced parallelization techniques. The deployment leverages OpenMP (Open Multi-Processing) for shared-memory parallelism within a single instance and MPI (Message Passing Interface) for distributed computing across multiple EC2 instances.\n\nThe system demonstrates high-performance computing techniques applied to AI inference, enabling efficient scaling of LLMs across cloud infrastructure. By combining both parallelism approaches, the deployment achieves optimal resource utilization and reduced inference latency compared to single-threaded or single-node approaches.\n\nThe deployment pipeline includes model optimization, environment configuration, distributed setup, and monitoring. This hands-on experience with cloud infrastructure and parallel computing provided valuable insights into production LLM deployment at scale.',
    howItWorks: '1. EC2 Provisioning: Launch multiple EC2 instances with appropriate compute and memory configurations.\n\n2. Environment Setup: Install required dependencies (Python, MPI libraries, deep learning frameworks).\n\n3. Model Optimization: Optimize LLM for distributed inference (model sharding, quantization).\n\n4. OpenMP Configuration: Configure thread-based parallelism within each instance for multi-core utilization.\n\n5. MPI Setup: Configure MPI for inter-node communication and distributed processing.\n\n6. Load Distribution: Distribute model layers and workload across available instances.\n\n7. Inference Pipeline: Coordinate parallel execution for efficient inference.\n\n8. Monitoring: Track resource utilization, latency, and throughput metrics.',
    keyFeatures: [
      'Hybrid Parallelism: Combines OpenMP (shared memory) and MPI (distributed) for optimal performance',
      'Cloud Infrastructure: Hands-on experience with AWS EC2 deployment and configuration',
      'Scalable Architecture: Can scale across multiple instances for larger models',
      'Resource Optimization: Efficient use of compute resources through parallel processing',
      'Production Deployment: Real-world experience deploying LLMs in cloud environment',
      'Performance Monitoring: Tools for tracking inference latency and resource utilization'
    ],
    category: 'Cloud',
    status: 'Completed',
    techStack: ['AWS EC2', 'Python', 'OpenMP', 'MPI'],
    year: '2024',
    period: '2024',
  },
]
